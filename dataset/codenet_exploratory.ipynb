{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5defae48",
   "metadata": {},
   "source": [
    "# Load the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "105dfe2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harshit/Library/Caches/pypoetry/virtualenvs/ai-code-reviewer-e5MqfTei-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load the canonical dataset: 'code_x_glue_cc_defect_detection'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 5.60k/5.60k [00:00<00:00, 6.14kB/s]\n",
      "Downloading data: 100%|██████████| 17.8M/17.8M [00:06<00:00, 2.91MB/s]\n",
      "Downloading data: 100%|██████████| 2.21M/2.21M [00:01<00:00, 1.86MB/s]\n",
      "Downloading data: 100%|██████████| 2.23M/2.23M [00:01<00:00, 2.19MB/s]\n",
      "Generating train split: 100%|██████████| 21854/21854 [00:00<00:00, 247718.59 examples/s]\n",
      "Generating validation split: 100%|██████████| 2732/2732 [00:00<00:00, 238184.92 examples/s]\n",
      "Generating test split: 100%|██████████| 2732/2732 [00:00<00:00, 254138.23 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ OFFICIAL BENCHMARK DATASET LOADED SUCCESSFULLY!\n",
      "--- Dataset Structure ---\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'func', 'target', 'project', 'commit_id'],\n",
      "        num_rows: 21854\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'func', 'target', 'project', 'commit_id'],\n",
      "        num_rows: 2732\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'func', 'target', 'project', 'commit_id'],\n",
      "        num_rows: 2732\n",
      "    })\n",
      "})\n",
      "\n",
      "First 5 rows:\n",
      "   id                                               func  target project  \\\n",
      "0   0  static av_cold int vdadec_init(AVCodecContext ...   False  FFmpeg   \n",
      "1   1  static int transcode(AVFormatContext **output_...   False  FFmpeg   \n",
      "2   2  static void v4l2_free_buffer(void *opaque, uin...   False  FFmpeg   \n",
      "3   4  int av_opencl_buffer_write(cl_mem dst_cl_buf, ...   False  FFmpeg   \n",
      "4   5  static int r3d_read_rdvo(AVFormatContext *s, A...    True  FFmpeg   \n",
      "\n",
      "                                  commit_id  \n",
      "0  973b1a6b9070e2bf17d17568cbaf4043ce931f51  \n",
      "1  321b2a9ded0468670b7678b7c098886930ae16b2  \n",
      "2  5d5de3eba4c7890c2e8077f5b4ae569671d11cf8  \n",
      "3  57d77b3963ce1023eaf5ada8cba58b9379405cc8  \n",
      "4  aba232cfa9b193604ed98f3fa505378d006b1b3b  \n",
      "\n",
      "Label distribution:\n",
      "target\n",
      "False    11836\n",
      "True     10018\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# The official, stable CodeXGLUE dataset from Microsoft for Defect Detection\n",
    "dataset_name = \"code_x_glue_cc_defect_detection\"\n",
    "\n",
    "print(f\"Attempting to load the canonical dataset: '{dataset_name}'\")\n",
    "\n",
    "try:\n",
    "    # This dataset is large, so the download may take a moment\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    \n",
    "    print(\"\\n✅ OFFICIAL BENCHMARK DATASET LOADED SUCCESSFULLY!\")\n",
    "    print(\"--- Dataset Structure ---\")\n",
    "    print(dataset)\n",
    "    \n",
    "    # Explore the training data\n",
    "    df = dataset['train'].to_pandas()\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Check the label column, which is named 'target'\n",
    "    # 0 = correct, 1 = incorrect/defective\n",
    "    print(\"\\nLabel distribution:\")\n",
    "    print(df['target'].value_counts())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ An error occurred during loading: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c20f791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer \n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07b4d50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tokenization function \n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Applies the tokenizer to a batch of code examples from the 'func' column.\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"func\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc6adc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 21854/21854 [01:40<00:00, 217.98 examples/s]\n",
      "Map: 100%|██████████| 2732/2732 [00:12<00:00, 218.02 examples/s]\n",
      "Map: 100%|██████████| 2732/2732 [00:12<00:00, 215.31 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# apply the function to the dataset \n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32ac7138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor(False), 'input_ids': tensor([    0, 42653,  6402,  1215, 33912,  6979,   748,   417,  1829,   438,\n",
      "         1215, 25153,  1640, 10612, 47436,  3204, 48522,  1009,  1469, 49575,\n",
      "           43, 50118, 50118, 45152, 50140,  1437,  1437,  1437,   468,   495,\n",
      "         2606,  3204, 15362, 48522,  1009, 49575,  5457,  6402, 49575, 46613,\n",
      "        25943,  1215, 23687,   131, 50140,  1437,  1437,  1437, 29916,   748,\n",
      "         6106,  1215, 46796,  1009,   705,  6106,  1215, 49575,  5457,   359,\n",
      "        49575, 46613,   705,  6106,  1215, 49575,   131, 50140,  1437,  1437,\n",
      "         1437,  8192, 47731,  2194,   131, 50140,  1437,  1437,  1437,  6979,\n",
      "         5494,   131, 50140, 50140,  1437,  1437,  1437,   740, 43820, 46613,\n",
      "          298, 29137,  1215, 49722,  5457,   321,   131, 50140, 50140,  1437,\n",
      "         1437,  1437, 48565, 45511,   181,  3181,  1215, 40523,  1872,     9,\n",
      "        45797, 48404, 50140,  1437,  1437,  1437,   114, 48209,  3145,  1215,\n",
      "          298, 29137,  1215,   705,  6106,  1215, 11127, 15362,     4,   642,\n",
      "         3181,  1215, 40523,  1872,    43, 25522, 50140,  1437,  1437,  1437,\n",
      "         1437,  1437,  1437,  1437,   114,    36,   330,   347,  5268,  1688,\n",
      "        29991,  1258, 47322, 43623, 28696,   449,   347,  5268,  1688, 29991,\n",
      "         1258, 47322, 43623,   698,  1215,   406,    43, 50140,  1437,  1437,\n",
      "         1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437, 48400,\n",
      "         1215,   298, 29137,  1215,   705,  6106,  1215, 11127, 15362,     4,\n",
      "          642,  3181,  1215, 40523,  1872,  5457,   748,  6106,  1215,   642,\n",
      "         3181, 40523,  1872,  1215, 13718,   368,  1215,   698,  1215,   406,\n",
      "          131, 50140,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1493,\n",
      "        50140,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "         1437,  1437, 48400,  1215,   298, 29137,  1215,   705,  6106,  1215,\n",
      "        11127, 15362,     4,   642,  3181,  1215, 40523,  1872,  5457,   748,\n",
      "         6106,  1215,   642,  3181, 40523,  1872,   131, 50140,  1437,  1437,\n",
      "         1437, 35524, 50140, 50140,  1437,  1437,  1437, 48565, 45511,   748,\n",
      "         6106, 48404, 50140,  1437,  1437,  1437, 26012,  8738,  1640,   705,\n",
      "         6106,  1215, 49575,     6,   321,     6, 49907,  1640, 25384,   748,\n",
      "         6106,  1215, 46796, 48749, 50140,  1437,  1437,  1437,   748,  6106,\n",
      "         1215, 49575, 46613, 36097,  5457,  6402, 49575, 46613, 36097,   131,\n",
      "        50140,  1437,  1437,  1437,   748,  6106,  1215, 49575, 46613, 37009,\n",
      "         5457,  6402, 49575, 46613, 37009,   131, 50140,  1437,  1437,  1437,\n",
      "          748,  6106,  1215, 49575, 46613, 34609,  5457,   128,  1469,   438,\n",
      "          134, 23500, 50140,  1437,  1437,  1437,   748,  6106,  1215, 49575,\n",
      "        46613,  3698,  1215, 45176,  1215, 11127, 19519,  5457,   112,   131,\n",
      "        50140,  1437,  1437,  1437,   748,  6106,  1215, 49575, 46613,  3698,\n",
      "         1215, 13043,  1215, 47438,  5457,   112,   131, 50140,  1437,  1437,\n",
      "         1437,   740, 43820, 46613,   642,  3181,  1215,   506, 16100,  5457,\n",
      "         6402, 49575, 46613,  6460,  1215, 34609,  1640,  1469, 49575,     6,\n",
      "         6402, 49575, 46613, 29659,  3204, 46613,   642,  3181,  1215, 40523,\n",
      "         1872,  4397, 50140,  1437,  1437,  1437,  5405,    36, 49575, 46613,\n",
      "          642,  3181,  1215,   506, 16100,    43, 25522, 50140,  1437,  1437,\n",
      "         1437,   403, 17307,  1215,   510,  9482,  1215,   597, 11674,  1215,\n",
      "          791,   975,   846,   975, 37319,    35, 50140,  1437,  1437,  1437,\n",
      "         1437,  1437,  1437,  1437,   748,  6106,  1215, 49575, 46613, 38635,\n",
      "         1215,   642,  3181,  1215,   506, 16100,  1215, 12528,  5457,   128,\n",
      "          176,   705,  5781, 23500, 50140,  1437,  1437,  1437,  1437,  1437,\n",
      "         1437,  1437,  1108,   131, 50140,  1437,  1437,  1437,   403, 17307,\n",
      "         1215,   510,  9482,  1215,   597, 11674,  1215,   975,   791,   975,\n",
      "          846, 37319,    35, 50140,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "         1437,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "# Rename the 'target' column to 'labels' for the trainer\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"target\", \"labels\")\n",
    "\n",
    "# Remove the original columns that the model doesn't need for training\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"id\", \"func\", \"project\", \"commit_id\"])\n",
    "\n",
    "# Set the format of the dataset to PyTorch tensors\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Let's inspect the final result of one example\n",
    "print(tokenized_datasets['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ccfbae",
   "metadata": {},
   "source": [
    "# Load the Pre Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "632a0568",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/codebert-base\", \n",
    "    num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f46af6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# define training arguments\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments\n\u001b[0;32m----> 5\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./kodo-codebert-finetuned-defect\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Directory to save the trained model\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# Evaluate performance at the end of each epoch\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                   \u001b[49m\u001b[38;5;66;43;03m# We'll start with 1 epoch for a quick first run\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Number of examples per batch for training\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# Number of examples per batch for evaluation\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                    \u001b[49m\u001b[38;5;66;43;03m# Log training progress every 100 steps\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# Save a checkpoint at the end of each epoch\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# Load the best model at the end of training\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "# define training arguments\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./kodo-codebert-finetuned-defect\", # Directory to save the trained model\n",
    "    evaluation_strategy=\"epoch\",          # Evaluate performance at the end of each epoch\n",
    "    num_train_epochs=1,                   # We'll start with 1 epoch for a quick first run\n",
    "    per_device_train_batch_size=8,        # Number of examples per batch for training\n",
    "    per_device_eval_batch_size=8,         # Number of examples per batch for evaluation\n",
    "    logging_steps=100,                    # Log training progress every 100 steps\n",
    "    save_strategy=\"epoch\",                # Save a checkpoint at the end of each epoch\n",
    "    load_best_model_at_end=True,          # Load the best model at the end of training\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-code-reviewer-e5MqfTei-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
